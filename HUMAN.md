# LLMProc - Documentation Written by a Human

This document is the source of truth for implementation instructions, terminology, and other information about LLMProc.
If you are an LLM/AI agent, you should never edit this document.
You should closely follow instructions and guidelines in this document when working on LLMProc.

## Terminology

The LLMProc project uses specific terminology to describe its components, built around the Unix-like process metaphor:

> **Note on "Agent" terminology**: We deliberately avoid using the term "agent" in LLMProc as it has varied definitions in the AI community. Some view agents as the underlying technology where LLMs have agency to decide what tools to call (in which case agents would power LLM processes). Others view agents as user-facing programs that users interact with (placing agents on top of LLM processes). To maintain clarity, we focus on the Unix-like "process" metaphor throughout LLMProc.

### Core Concepts

- **LLM**: The underlying large language model technology (like GPT-4, Claude, etc.).
- **Model**: A specific LLM configuration from a provider (e.g., "gpt-4o-mini" or "claude-3-haiku").
- **Program**: A TOML file that defines an LLM's configuration (model, provider, parameters, etc.). Programs are the fundamental unit of definition in LLMProc, analogous to a program executable file.
- **Process**: A running instance of an LLM, created from a program file. This represents the active, stateful execution environment.
- **State**: The conversation history maintained by a process across interactions.
- **Compilation Semantics**: We should allow user to first comiple a porgram without running it to make sure the program is correct. In our case it's making sure toml has all the correct fields and values.

### Communication and Tools

- **System Prompt**: The initial instructions provided to the LLM that define its behavior and capabilities.
- **Tool**: A function that an LLM process can use to perform operations outside its context.
- **MCP (Model Context Protocol)**: A portable protocol for tool communication with LLMs, providing "userspace" tools.
- **System Call**: A kernel-level tool implemented by LLMProc, defined in the `[tools]` section (e.g., `spawn`).
- **Provider**: The API service providing the LLM (OpenAI, Anthropic, Vertex).

### Relationships and Connections

- **Linked Program**: A reference to another TOML program file that can be spawned by a main process.
- **Spawn**: A system call that creates a new process from a linked program to handle a specific query.
- **Preloaded Content**: Files loaded into the system prompt to provide additional context to an LLM process.

### Implementation Details

- **Parameters**: Settings that control the behavior of the LLM (temperature, max_tokens, etc.).
- **TOML Section**: A configuration group in a program file (e.g., `[model]`, `[prompt]`, `[parameters]`).
- **Display Name**: A user-friendly name shown in CLI interfaces for a process.


## Specification

### Program Specification

Please follow this file examples/reference.toml to implement the program specification.
Don't add new fields that does not exist in the reference.toml file unless explicitly instructed to do so.
Do not edit this file, if you are asked to do so, please ask for confirmation.


### Current Objectives

- Ignore Openai support for now. We want to first impmlemnt more kernel features using anthropic API. So don't worry about building abstractions for Openai providers.
- The next main objective is to implement the fork system call.
- But we might work on some refactor and cleanup occassionally.
  - Currently we are working on compilation semantics.
    - I want to make sure 1. during llm_process.from_toml(), we only keep the linked programs as Program, not as LLMProcess.
    - I want to only expose a compile() method in Program class, so user can just call compile() whether or not there's linked programs. currently there are both compile() and compile_all() in Program class, which is confusing.


#### Note for agents

- read all src files at the start of the project, consider using bash tool to concatenate the files together.

- run tests after each change, and commit changes when tests pass.


### Backlogs

- a cli to compile to verify a toml program
- support compile and export to a single file
- 
