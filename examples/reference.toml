# LLMProc Reference Program
# All supported options and parameters with minimal examples

#----------------------------------------
# Model Configuration
#----------------------------------------
[model]
# Model name (Required)
name = "claude-3-7-sonnet-20250219"  # Default Anthropic model
# name = "claude-3-5-sonnet-20241022"  # Alternative Claude model
# name = "claude-3-haiku-20240307"  # Faster, cheaper Claude model
# name = "gpt-4o"  # OpenAI example

# Provider (Required)
provider = "anthropic"  # Options: "anthropic", "anthropic_vertex", "openai"
# provider = "anthropic_vertex"  # For Claude models via Google Vertex AI (uses ANTHROPIC_VERTEX_PROJECT_ID, CLOUD_ML_REGION env vars)
# provider = "openai"  # For OpenAI models

# Optional: User-facing display name for the model
# This is used in CLI interfaces and doesn't affect API calls
# If not provided, defaults to "{provider} {model_name}"
display_name = "Claude 3.7 Sonnet"

# Optional: Disable automatic prompt caching (Anthropic models only)
# Prompt caching is automatically enabled for all Anthropic models
# This can reduce token usage by up to 90% and improve response times
# Set to true only if you need to explicitly disable caching
disable_automatic_caching = false

#----------------------------------------
# Generation Parameters
#----------------------------------------
[parameters]
# Controls randomness (0.0-1.0 for Anthropic, 0.0-2.0 for OpenAI)
temperature = 0.7

# Maximum tokens to generate
max_tokens = 4096  # Claude 3.7 supports up to 32,768 tokens

# Nucleus sampling parameter (0.0-1.0)
top_p = 0.95

# Top-K sampling (Anthropic only)
top_k = 40

# Claude 3.7 thinking parameter (controls depth of reasoning, Anthropic only)
# Configure how Claude uses its thinking capability to solve complex problems
[parameters.thinking]
type = "enabled"  # "enabled" or "disabled"
budget_tokens = 4096  # Budget for thinking in tokens (min 1024, recommended 4096-8192)

# Token-efficient tool use for Claude 3.7 (Anthropic only)
# Automatically added for appropriate models to reduce token usage
[parameters.extra_headers]
# anthropic-beta = "token-efficient-tools-2025-02-19"  # Uncomment for token-efficient tool calls

# OpenAI-specific parameters (only used with OpenAI provider)
# frequency_penalty = 0.0  # -2.0 to 2.0, penalizes repeated tokens
# presence_penalty = 0.0  # -2.0 to 2.0, penalizes tokens already present

# Note: Prompt caching is automatically enabled for Anthropic models
# No extra configuration needed - reduces token usage by up to 90%

#----------------------------------------
# Prompt Configuration
#----------------------------------------
[prompt]
# System prompt as a string
# This is the base system prompt without any enhancements
system_prompt = """You are a helpful assistant.
Provide clear, accurate, and concise responses to user queries.
Always maintain a neutral and professional tone."""

# OR use a file (relative path, takes precedence over system_prompt)
# system_prompt_file = "../prompts/my_system_prompt.md"

# Notes on system prompts:
# - The system_prompt here is the base instruction set
# - The actual prompt sent to the API is an "enriched system prompt"
#   that includes environment info and preloaded file content
# - The enriched prompt is generated on first run and preserved for the session

#----------------------------------------
# Preload Configuration
#----------------------------------------
[preload]
# Files to preload as context (relative paths)
# Content will be added to the enriched system prompt with XML tags
# files = [
#   "../prompts/example_prompt.md",
#   "../README.md"
# ]
#
# When files are preloaded:
# - On first message, they're added to the enriched system prompt and stay fixed even if file changed during the conversation
# - Format: <preload><file path="filename">content</file></preload>
# - The combined system prompt + preload is used for all messages in the session

#----------------------------------------
# Environment Info Configuration
#----------------------------------------
[env_info]
# The environment information feature provides context about the runtime environment to the LLM.
# This adds an <env> XML block to the system prompt with selected information.
#
# This is disabled by default for privacy/security reasons (opt-in model).
# No environment information is shared unless explicitly configured.
#
# CONFIGURATION OPTIONS:
#
# 1. SPECIFIC VARIABLES: List the exact variables you want included
variables = ["working_directory", "platform", "date"]  

# 2. ALL VARIABLES: Use "all" to include all standard environment variables
# variables = "all"  

# 3. DISABLE: Use an empty list to explicitly disable (this is the default)
# variables = []  

# NOTE: Only system-defined variables are supported.
# Custom variables are not allowed in the env_info section.

# AVAILABLE STANDARD VARIABLES:
# - working_directory: Current working directory path
#   Security note: Reveals file system structure
#
# - platform: Operating system (linux, darwin, windows)
#   Usage: Useful for OS-specific instructions
#
# - date: Current date in YYYY-MM-DD format
#   Usage: Provides temporal context for the model
#
# - python_version: Python interpreter version
#   Usage: Helpful for code generation and compatibility
#
# - hostname: Name of the machine
#   Security note: May reveal identifying information
#
# - username: Current user's system username
#   Security note: Reveals identity information
#
# SECURITY CONSIDERATIONS:
# - Only include variables necessary for your application
# - Be cautious about exposing usernames, hostnames, or file paths
# - Never include API keys, tokens, or passwords as custom variables
# - Consider using different configurations for dev vs. production

# For full documentation, see docs/env_info.md

#----------------------------------------
# Tools Configuration
#----------------------------------------
[tools]
# List of enabled tools
enabled = ["calculator", "fork", "spawn", "read_fd"]  # Enable multiple tools

# Available tools:
# - calculator: Perform mathematical calculations
# - fork: Create process copies with the same state for parallel exploration
# - spawn: Create new processes from linked programs for specialized tasks
# - read_fd: Read content from file descriptors with pagination
# - fd_to_file: Export file descriptor content to local files

#----------------------------------------
# Program Linking Configuration
#----------------------------------------
[linked_programs]
# Map program names to their TOML program files
repo_expert = "./program_linking/repo_expert.toml"  # Program for repo expertise
code_assistant = "./basic/claude-3-haiku.toml"      # Program for coding help

#----------------------------------------
# MCP (Model Context Protocol) Configuration
#----------------------------------------
[mcp]
# Path to the MCP servers configuration file
# config_path = "config/mcp_servers.json"

# Tool Configuration
# Users must explicitly specify which tools to import from each server
[mcp.tools]
# Server name = List of specific tools to import OR "all" to import all tools
sequential-thinking = "all"  # Sequential thinking for step-by-step reasoning
# Other servers (commented out as they're not in current config)
# github = ["search_repositories", "get_file_contents"]  # Specific GitHub tools
# exa = ["search"]                                      # Exa search tool
# code_tools = "all"                                    # All code tools
# webfetch = ["fetch"]                                  # Web fetch tools

#----------------------------------------
# File Descriptor Configuration
#----------------------------------------
[file_descriptor]
# Enable file descriptor system explicitly (also enabled if read_fd or fd_to_file in tools.enabled)
enabled = true

# Threshold for FD creation (larger than page size)
max_direct_output_chars = 8000

# Page size for pagination
default_page_size = 4000

# Threshold for user input FD creation (Phase 3 feature)
max_input_chars = 8000

# Enable/disable user input paging (Phase 3 feature)
page_user_input = true

# TOOLS:
# - read_fd: Read file descriptor content with pagination support
#   read_fd(fd="fd:1", start=2) - Read specific page
#   read_fd(fd="fd:1", read_all=True) - Read all content
#   read_fd(fd="fd:1", start=2, extract_to_new_fd=True) - Extract page to new FD
#   read_fd(fd="fd:1", mode="line", start=10, count=5) - Read lines 10-14
#   read_fd(fd="fd:1", mode="char", start=100, count=50) - Read 50 characters starting at position 100
#
# - fd_to_file: Export file descriptor content to a file
#   Basic usage:
#   fd_to_file(fd="fd:1", file_path="/path/to/output.txt")
#   
#   Enhanced options:
#   fd_to_file(fd="fd:1", file_path="file.txt", mode="append") - Append to file
#   fd_to_file(fd="fd:1", file_path="file.txt", exist_ok=False) - Create only if doesn't exist
#   fd_to_file(fd="fd:1", file_path="file.txt", create=False) - Update existing only
#   
#   Behavior matrix:
#   - mode="write", create=True, exist_ok=True: Create or overwrite (default)
#   - mode="write", create=True, exist_ok=False: Create only if doesn't exist
#   - mode="write", create=False, exist_ok=True: Update existing only
#   - mode="append", create=True, exist_ok=True: Append, create if needed
#   - mode="append", create=True, exist_ok=False: Append only if exists, else create new
#   - mode="append", create=False, exist_ok=True: Append to existing only
#   
#   Parent directories are created automatically if needed
#
# REFERENCE ID SYSTEM:
# Enable the reference ID system to allow LLMs to mark sections of their responses:
enable_references = true

# Reference ID System Features:
# - LLM can mark important sections with <ref id="unique_id">content</ref> tags
# - References are automatically stored in the file descriptor system
# - References can be accessed using "ref:" prefix (e.g., "ref:code_example")
# - Standard FD tools can be used with references:
#   * read_fd(fd="ref:code_example", read_all=true)
#   * fd_to_file(fd="ref:code_example", file_path="code.py")
# - References persist across conversation turns
# - References can be shared with other processes via spawn

