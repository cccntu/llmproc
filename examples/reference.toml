# LLMProc Reference Configuration
# All supported options and parameters with minimal examples

#----------------------------------------
# Model Configuration
#----------------------------------------
[model]
# Model name (Required)
name = "gpt-4o"  # OpenAI example
# provider = "anthropic"  # For Claude models directly via Anthropic API
# provider = "anthropic_vertex"  # For Claude models via Google Vertex AI

# Provider (Required)
provider = "openai"  # Options: "openai", "anthropic", "anthropic_vertex"

# Optional: User-facing display name for the model
# This is used in CLI interfaces and doesn't affect API calls
# If not provided, defaults to "{provider} {model_name}"
display_name = "GPT-4o Assistant"

#----------------------------------------
# Generation Parameters
#----------------------------------------
[parameters]
# Controls randomness (0.0-2.0 for OpenAI, 0.0-1.0 for Anthropic)
temperature = 0.7

# Maximum tokens to generate
max_tokens = 150

# Nucleus sampling parameter (0.0-1.0)
top_p = 0.95

# Top-K sampling (Anthropic only)
# top_k = 40

# Frequency penalty (OpenAI only, -2.0 to 2.0)
frequency_penalty = 0.0

# Presence penalty (OpenAI only, -2.0 to 2.0)
presence_penalty = 0.0

#----------------------------------------
# Prompt Configuration
#----------------------------------------
[prompt]
# System prompt as a string
# This is the base system prompt without any enhancements
system_prompt = """You are a helpful assistant.
Provide clear, accurate, and concise responses to user queries.
Always maintain a neutral and professional tone."""

# OR use a file (relative path, takes precedence over system_prompt)
# system_prompt_file = "../prompts/my_system_prompt.md"

# Notes on system prompts:
# - The system_prompt here is the base instruction set
# - The actual prompt sent to the API is an "enriched system prompt"
#   that includes environment info and preloaded file content
# - The enriched prompt is generated on first run and preserved for the session

#----------------------------------------
# Preload Configuration
#----------------------------------------
[preload]
# Files to preload as context (relative paths)
# Content will be added to the enriched system prompt with XML tags
# files = [
#   "../prompts/example_prompt.md",
#   "../README.md"
# ]
#
# When files are preloaded:
# - On first message, they're added to the enriched system prompt and stay fixed even if file changed during the conversation
# - Format: <preload><file path="filename">content</file></preload>
# - The combined system prompt + preload is used for all messages in the session

#----------------------------------------
# Environment Info Configuration
#----------------------------------------
[env_info]
# Control environment information included in the enriched system prompt
# By default, no environment information is included
# Use a list of variables to include or "all" to include all available variables
variables = ["working_directory", "platform", "date"]  # Include specific variables
# variables = "all"  # Include all available environment variables

# Custom environment variables can be added as string key-value pairs
# git_branch = "main"  # Custom git branch info
# project_name = "My Project"  # Custom project name

# Available standard variables:
# - working_directory: Current working directory
# - platform: Operating system (linux, darwin, windows)
# - date: Current date (YYYY-MM-DD)
# - python_version: Python version
# - hostname: Name of the machine
# - username: Current user

#----------------------------------------
# Tools Configuration
#----------------------------------------
[tools]
# List of enabled tools
# enabled = ["spawn"]  # Enable the spawn tool for program linking
# enabled = ["fork"]   # Enable the fork tool for creating copies of the current process
# enabled = ["spawn", "fork"]  # Enable multiple tools

#----------------------------------------
# Program Linking Configuration
#----------------------------------------
[linked_programs]
# Map program names to their TOML configuration files
# repo_expert = "./repo_expert.toml"
# code_helper = "./code_helper.toml"

#----------------------------------------
# MCP (Model Context Protocol) Configuration
#----------------------------------------
[mcp]
# Path to the MCP servers configuration file
# config_path = "config/mcp_servers.json"

# Tool Configuration
# Users must explicitly specify which tools to import from each server
[mcp.tools]
# Server name = List of specific tools to import OR "all" to import all tools
# github = ["search_repositories", "get_file_contents"]
# codemcp = ["ReadFile"]
# github = "all"  # Import all tools from GitHub server

#----------------------------------------
# Debug Configuration
#----------------------------------------
[debug]
# Enable detailed debugging output for tool execution
# debug_tools = true  # Set to true for verbose tool execution logging
