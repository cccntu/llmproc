This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
examples/
  complex.toml
  minimal.toml
llmproc/
  __init__.py
  llm_process.py
prompts/
  example_prompt.md
example.py
pyproject.toml
README.md
requirements.txt

================================================================
Files
================================================================

================
File: examples/complex.toml
================
[model]
name = "gpt-4o"
provider = "openai"
temperature = 0.7
max_tokens = 1000

[prompt]
system_prompt_file = "prompts/example_prompt.md"

[parameters]
top_p = 0.95
frequency_penalty = 0.0
presence_penalty = 0.0

================
File: examples/minimal.toml
================
[model]
name = "gpt-4o-mini"
provider = "openai"

[prompt]
system_prompt = "You are a helpful assistant."

================
File: llmproc/__init__.py
================
from .llm_process import LLMProcess

__all__ = ["LLMProcess"]

================
File: llmproc/llm_process.py
================
import tomllib
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()

class LLMProcess:
    def __init__(self, model_name, provider, system_prompt, **kwargs):
        if provider != "openai":
            raise NotImplementedError(f"Provider {provider} not implemented.")

        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model_name = model_name
        self.system_prompt = system_prompt
        self.state = [{"role": "system", "content": self.system_prompt}]
        self.parameters = kwargs

    @classmethod
    def from_toml(cls, toml_path):
        path = Path(toml_path)
        with path.open('rb') as f:
            config = tomllib.load(f)

        model = config['model']
        prompt_config = config.get('prompt', {})
        parameters = config.get('parameters', {})
        
        # Add any model-specific parameters from the model section
        model_params = {k: v for k, v in model.items() 
                       if k not in ['name', 'provider']}
        parameters.update(model_params)

        if 'system_prompt_file' in prompt_config:
            system_prompt_path = path.parent / prompt_config['system_prompt_file']
            system_prompt = system_prompt_path.read_text()
        else:
            system_prompt = prompt_config.get('system_prompt', '')

        return cls(
            model_name=model['name'],
            provider=model['provider'],
            system_prompt=system_prompt,
            **parameters
        )

    def run(self, user_input):
        self.state.append({"role": "user", "content": user_input})
        
        # Extract parameters for the API call
        temperature = self.parameters.get('temperature', 0.7)
        max_tokens = self.parameters.get('max_tokens', None)
        top_p = self.parameters.get('top_p', None)
        frequency_penalty = self.parameters.get('frequency_penalty', None)
        presence_penalty = self.parameters.get('presence_penalty', None)
        
        # Build kwargs with non-None parameters only
        kwargs = {
            'model': self.model_name,
            'messages': self.state,
            'temperature': temperature
        }
        
        if max_tokens is not None:
            kwargs['max_tokens'] = max_tokens
        if top_p is not None:
            kwargs['top_p'] = top_p
        if frequency_penalty is not None:
            kwargs['frequency_penalty'] = frequency_penalty
        if presence_penalty is not None:
            kwargs['presence_penalty'] = presence_penalty
            
        response = self.client.chat.completions.create(**kwargs)
        output = response.choices[0].message.content.strip()
        self.state.append({"role": "assistant", "content": output})
        return output
        
    def get_state(self):
        """Return the current conversation state"""
        return self.state.copy()
        
    def reset_state(self, keep_system_prompt=True):
        """Reset the conversation state, optionally keeping the system prompt"""
        if keep_system_prompt:
            self.state = [{"role": "system", "content": self.system_prompt}]
        else:
            self.state = []

================
File: prompts/example_prompt.md
================
You are a knowledgeable assistant specialized in technical explanations. Your responses should be:

1. Clear and concise
2. Technically accurate
3. Easy to understand for beginners
4. Comprehensive enough for experts

When providing code examples, include comments and explain your implementation choices. If you're unsure about something, acknowledge the limitations of your knowledge rather than making up information.

================
File: example.py
================
from llmproc import LLMProcess

def main():
    # Example with minimal configuration
    print("Running minimal example...")
    process_minimal = LLMProcess.from_toml('examples/minimal.toml')
    
    output = process_minimal.run('Hello!')
    print(f"Response: {output}\n")
    
    output = process_minimal.run('Explain quantum computing simply.')
    print(f"Response: {output}\n")
    
    print("-" * 50)
    
    # Example with complex configuration
    print("\nRunning complex example...")
    process_complex = LLMProcess.from_toml('examples/complex.toml')
    
    output = process_complex.run('What is machine learning?')
    print(f"Response: {output}\n")
    
    print("Conversation state:")
    for message in process_complex.get_state():
        print(f"[{message['role']}]: {message['content'][:50]}...")
    
    # Reset state
    print("\nResetting conversation state...")
    process_complex.reset_state()
    
    output = process_complex.run('How does a neural network work?')
    print(f"Response after reset: {output}")

if __name__ == "__main__":
    main()

================
File: pyproject.toml
================
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "llmproc"
version = "0.1.0"
authors = [
    {name = "Developer", email = "dev@example.com"},
]
description = "A simple framework for LLM-powered applications"
readme = "README.md"
requires-python = ">=3.11"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "openai>=1.0.0",
    "python-dotenv>=0.19.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "black>=23.0.0",
    "isort>=5.0.0",
]

[tool.setuptools]
packages = ["llmproc"]

[tool.black]
line-length = 88
target-version = ["py311"]

[tool.isort]
profile = "black"
line_length = 88

================
File: README.md
================
# LLMProc

A simple, flexible framework for building LLM-powered applications with a standardized configuration approach.

## Features

- Load configurations from TOML files
- Maintain conversation state
- Support for different LLM providers (OpenAI initially)
- Parameter customization
- Simple API for easy integration

## Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/llmproc.git
cd llmproc

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env with your API keys
```

## Usage

### Basic Example

```python
from llmproc import LLMProcess

# Load configuration from TOML
process = LLMProcess.from_toml('examples/minimal.toml')

# Run the process with user input
output = process.run('Hello!')
print(output)

# Continue the conversation
output = process.run('Tell me more about that.')
print(output)

# Reset the conversation state
process.reset_state()
```

### TOML Configuration

Minimal example:

```toml
[model]
name = "gpt-4o-mini"
provider = "openai"

[prompt]
system_prompt = "You are a helpful assistant."
```

Complex example:

```toml
[model]
name = "gpt-4o"
provider = "openai"
temperature = 0.7
max_tokens = 1000

[prompt]
system_prompt_file = "prompts/example_prompt.md"

[parameters]
top_p = 0.95
frequency_penalty = 0.0
presence_penalty = 0.0
```

## License

MIT

================
File: requirements.txt
================
openai>=1.0.0
python-dotenv>=0.19.0



================================================================
End of Codebase
================================================================
